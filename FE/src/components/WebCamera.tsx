import { useCallback, useEffect, useRef, useState } from "react";
import Webcam from "react-webcam";
import {
  HandLandmarker,
  HandLandmarkerResult,
  FilesetResolver
} from "@mediapipe/tasks-vision";

// const SERVER_WS_URL = "ws://localhost:8000/ws/predict"; // FastAPI WebSocket ì£¼ì†Œ

interface WebCameraProps {
  // ê°€ì´ë“œë¼ì¸ svg ì¡°ì ˆ props
  guidelineClassName?: string;
}

const WebCamera = ({ guidelineClassName }: WebCameraProps) => {
  const webcamRef = useRef<Webcam>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const handLandmarkerRef = useRef<HandLandmarker | null>(null);
  const resultsRef = useRef<HandLandmarkerResult | null>(null);
  const landmarkFrames = useRef<any[]>([]); // 50í”„ë ˆì„ ì €ì¥í•  ë°°ì—´
  const [gesture, setGesture] = useState<string | null>(null); // ì„œë²„ ì‘ë‹µëœ ì œìŠ¤ì²˜
  const [isConnected, setIsConnected] = useState(false); // WebSocket ì—°ê²° ìƒíƒœ
  const [isLoading, setIsLoading] = useState(true); // ëª¨ë¸ ë¡œë”© ìƒíƒœ
  const animationRef = useRef<number | null>(null);
  const socket = useRef<WebSocket | null>(null);

  // HandLandmarker ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
  const initializeHandLandmarker = async () => {
    const filesetResolver = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
    );
    
    const handLandmarker = await HandLandmarker.createFromOptions(filesetResolver, {
      baseOptions: {
        modelAssetPath: "https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/hand_landmarker.task",
        delegate: "GPU"
      },
      numHands: 2,
      runningMode: "VIDEO",
      minHandDetectionConfidence: 0.5,
      minHandPresenceConfidence: 0.5,
      minTrackingConfidence: 0.5,
      normRectCalculatorOptions: {
        useImageDimensions: true
      }
    });
    
    handLandmarkerRef.current = handLandmarker;
    setIsLoading(false);
  };

  // ì›¹ìº ì—ì„œ í”„ë ˆì„ì„ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
  const predictWebcam = useCallback(() => {
    if (
      !webcamRef.current ||
      !webcamRef.current.video ||
      !canvasRef.current ||
      !handLandmarkerRef.current ||
      webcamRef.current.video.readyState !== 4
    ) {
      // ì•„ì§ ì¤€ë¹„ê°€ ì•ˆ ë˜ì—ˆìœ¼ë©´ ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ë‹¤ì‹œ ì‹œë„
      animationRef.current = requestAnimationFrame(predictWebcam);
      return;
    }
    
    const video = webcamRef.current.video;
    const timestamp = performance.now();
    
    // ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ì† ëœë“œë§ˆí¬ ê°ì§€
    const results = handLandmarkerRef.current.detectForVideo(video, timestamp);
    resultsRef.current = results;
    
    // ì† ëœë“œë§ˆí¬ ì €ì¥
    if (results.landmarks && results.landmarks.length > 0) {
      landmarkFrames.current.push(results.landmarks);
    }
    
    // 50í”„ë ˆì„ì´ ëª¨ì´ë©´ ì„œë²„ë¡œ ì „ì†¡
    if (landmarkFrames.current.length === 50) {
      sendLandmarksToServer(landmarkFrames.current);
      landmarkFrames.current = []; // ì „ì†¡ í›„ ì´ˆê¸°í™”
    }
    
    // ìº”ë²„ìŠ¤ì— ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
    drawCanvas(results);
    
    // ë‹¤ìŒ í”„ë ˆì„ ì²˜ë¦¬
    animationRef.current = requestAnimationFrame(predictWebcam);
  }, []);

  // ìº”ë²„ìŠ¤ì— ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° í•¨ìˆ˜
  const drawCanvas = (results: HandLandmarkerResult) => {
    const canvasCtx = canvasRef.current!.getContext("2d")!;
    const width = canvasRef.current!.width;
    const height = canvasRef.current!.height;
    
    // ìº”ë²„ìŠ¤ ì´ˆê¸°í™”
    canvasCtx.save();
    canvasCtx.clearRect(0, 0, width, height);
    
    // ë¹„ë””ì˜¤ë¥¼ ìº”ë²„ìŠ¤ì— ê·¸ë¦¬ê¸° (ì¹´ë©”ë¼ í”¼ë“œ í‘œì‹œ)
    if (webcamRef.current && webcamRef.current.video) {
      canvasCtx.drawImage(
        webcamRef.current.video,
        0, 0, width, height
      );
    }
    
    // ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
    if (results.landmarks) {
      for (const landmarks of results.landmarks) {
        // ê´€ì ˆ ì—°ê²°ì„  ê·¸ë¦¬ê¸° (ì†ê°€ë½ê³¼ ì† ìœ¤ê³½)
        drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
          color: "#00FF00",
          lineWidth: 5
        });
        
        // ëœë“œë§ˆí¬ ì  ê·¸ë¦¬ê¸°
        drawLandmarks(canvasCtx, landmarks, {
          color: "#FF0000",
          lineWidth: 2,
          radius: 4
        });
      }
    }
    
    canvasCtx.restore();
  };

  // ì„œë²„ë¡œ 50í”„ë ˆì„ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜
  const sendLandmarksToServer = (frames: any[]) => {
    if (socket.current && socket.current.readyState === WebSocket.OPEN) {
      const data = JSON.stringify({ frames });
      socket.current.send(data);
      console.log("[ğŸ“¤ ì„œë²„ë¡œ 50í”„ë ˆì„ ì „ì†¡]");
    }
  };
  
  // WebSocket ì—°ê²° ê´€ë¦¬: isConnected ìƒíƒœì— ë”°ë¼ ì—°ê²° ìƒì„± ë° í•´ì œ
  useEffect(() => {
    if (isConnected) {
      socket.current = new WebSocket(import.meta.env.VITE_SERVER_WS_URL);

      socket.current.onopen = () => {
        console.log("[âœ… WebSocket ì—°ê²°ë¨]");
      };
      socket.current.onmessage = (event) => {
        try {
          const response = JSON.parse(event.data);
          setGesture(response.gesture); // ì„œë²„ì—ì„œ ë°›ì€ ì œìŠ¤ì²˜ í‘œì‹œ
        } catch (err) {
          console.error("ë©”ì‹œì§€ íŒŒì‹± ì˜¤ë¥˜:", event.data);
        }
      };
      socket.current.onclose = () => {
        console.log("[âŒ WebSocket ì—°ê²° ì¢…ë£Œ]");
      };
      socket.current.onerror = (error) => {
        console.error("[âš ï¸ WebSocket ì˜¤ë¥˜]", error);
      };
    } else {
      if (socket.current) {
        socket.current.close();
        socket.current = null;
      }
    }
    // ì—°ê²° ìƒíƒœê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ì‹¤í–‰
  }, [isConnected]);

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ HandLandmarker ì´ˆê¸°í™”
  useEffect(() => {
    initializeHandLandmarker();
    
    // ì»´í¬ë„ŒíŠ¸ê°€ ë§ˆìš´íŠ¸ë˜ë©´ ìë™ìœ¼ë¡œ ì—°ê²° ì‹œì‘
    setIsConnected(true);
    
    return () => {
      // ë¦¬ì†ŒìŠ¤ ì •ë¦¬
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
      }
      
      // WebSocket ì—°ê²° í•´ì œ
      if (socket.current) {
        socket.current.close();
        socket.current = null;
      }
    };
  }, []);
  
  // ëª¨ë¸ ë¡œë”©ì´ ì™„ë£Œë˜ë©´ ì›¹ìº  ì˜ˆì¸¡ ì‹œì‘
  useEffect(() => {
    if (!isLoading && handLandmarkerRef.current) {
      predictWebcam();
    }
  }, [isLoading, predictWebcam]);

  // ì† ì—°ê²° ì •ë³´ 
  const HAND_CONNECTIONS = [
    [0, 1], [1, 2], [2, 3], [3, 4], // ì—„ì§€
    [0, 5], [5, 6], [6, 7], [7, 8], // ê²€ì§€
    [5, 9], [9, 10], [10, 11], [11, 12], // ì¤‘ì§€
    [9, 13], [13, 14], [14, 15], [15, 16], // ì•½ì§€
    [13, 17], [17, 18], [18, 19], [19, 20], // ì†Œì§€
    [0, 17], [17, 5], [5, 0] // ì†ë°”ë‹¥
  ];
  
  // ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° í•¨ìˆ˜
  const drawLandmarks = (
    ctx: CanvasRenderingContext2D, 
    landmarks: any[], 
    options: { color: string; lineWidth: number; radius: number }
  ) => {
    const { color, lineWidth, radius } = options;
    
    for (const landmark of landmarks) {
      const x = landmark.x * ctx.canvas.width;
      const y = landmark.y * ctx.canvas.height;
      
      ctx.beginPath();
      ctx.arc(x, y, radius, 0, 2 * Math.PI);
      ctx.fillStyle = color;
      ctx.fill();
      ctx.lineWidth = lineWidth;
      ctx.strokeStyle = "#FFFFFF";
      ctx.stroke();
    }
  };
  
  // ì—°ê²°ì„  ê·¸ë¦¬ê¸° í•¨ìˆ˜
  const drawConnectors = (
    ctx: CanvasRenderingContext2D, 
    landmarks: any[], 
    connections: number[][], 
    options: { color: string; lineWidth: number }
  ) => {
    const { color, lineWidth } = options;
    
    ctx.lineWidth = lineWidth;
    ctx.strokeStyle = color;
    
    for (const connection of connections) {
      const [start, end] = connection;
      if (landmarks[start] && landmarks[end]) {
        const startX = landmarks[start].x * ctx.canvas.width;
        const startY = landmarks[start].y * ctx.canvas.height;
        const endX = landmarks[end].x * ctx.canvas.width;
        const endY = landmarks[end].y * ctx.canvas.height;
        
        ctx.beginPath();
        ctx.moveTo(startX, startY);
        ctx.lineTo(endX, endY);
        ctx.stroke();
      }
    }
  };

  return (
    <div className="w-full h-full bg-white relative overflow-hidden">
      {isLoading && (
        <div className="absolute inset-0 flex items-center justify-center bg-black bg-opacity-70 z-50">
          <div className="text-white text-xl font-bold">ëª¨ë¸ ë¡œë”© ì¤‘...</div>
        </div>
      )}
      
      {/* ì›¹ìº  (ìˆ¨ê²¨ì§„ ìƒíƒœ) */}
      <Webcam
        audio={false}
        width={1280}
        height={720}
        ref={webcamRef}
        videoConstraints={{
          facingMode: 'user',
        }}
        className="invisible absolute"
      />
      
      {/* ìº”ë²„ìŠ¤ (ì›¹ìº  í™”ë©´ê³¼ ì† ëœë“œë§ˆí¬ë¥¼ í‘œì‹œ) */}
      <canvas 
        ref={canvasRef} 
        width={1280} 
        height={720} 
        className="w-full h-full object-cover"
        style={{ transform: 'scaleX(-1)' }}
      />
      
      {/* ê°€ì´ë“œë¼ì¸ ì»¨í…Œì´ë„ˆ - ê¸°ì¡´ WebCamera ì»´í¬ë„ŒíŠ¸ì™€ ë™ì¼ */}
      <div className="absolute inset-0 flex justify-center items-center pointer-events-none">
        <div className="relative w-full h-[90%] flex justify-center items-center overflow-hidden">
          {/* SVG ê°€ì´ë“œë¼ì¸ */}
          <img 
            src="/images/guide-line.svg" 
            alt="ì¹´ë©”ë¼ ê°€ì´ë“œë¼ì¸" 
            className={`absolute ${guidelineClassName}`}
          />
          {/* ì•ˆë‚´ í…ìŠ¤íŠ¸ - ìœ„ì¹˜ ì¡°ì • */}
          <p 
            className="absolute top-5 text-center
            text-sm md:text-lg xl:text-xl font-[NanumSquareRoundEB] text-white
            drop-shadow-basic"
          >
            ì† ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í•˜ê³  ìˆìŠµë‹ˆë‹¤
          </p>
        </div>
      </div>
      
      {/* ì œìŠ¤ì²˜ ì¸ì‹ ê²°ê³¼ í‘œì‹œ (í™”ë©´ ìƒë‹¨ì— í‘œì‹œ) */}
      {gesture && (
        <div className="absolute top-20 left-0 right-0 flex justify-center items-center">
          <div className="bg-black bg-opacity-70 text-white px-4 py-2 rounded-lg font-bold">
            ì¸ì‹ëœ ì œìŠ¤ì²˜: {gesture}
          </div>
        </div>
      )}
    </div>
  );
};

export default WebCamera;